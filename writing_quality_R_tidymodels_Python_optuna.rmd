---
title: ''
author: "Arnold Kakas"
date: "`r Sys.Date()`"
output: html_document
---

<https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline> <https://www.kaggle.com/code/pehahn/xgb-base-in-r> <https://juliasilge.com/blog/xgboost-tune-volleyball/>

## Load packages and data

```{r}
pacman::p_load(tidyverse,
               tidymodels,
               bonsai,
               corrplot,
               ggplot2,
               plotly,
               vip,
               reticulate,
               doParallel,
               tictoc)

use_condaenv("my_conda_env")
```

```{r}
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list = ls(name = env), pos = env)
}
```

```{r}
# train_scores <- read_csv('../input/linking-writing-processes-to-writing-quality/train_scores.csv', show_col_types = FALSE)
# test_logs <- read_csv('../input/linking-writing-processes-to-writing-quality/test_logs.csv', show_col_types = FALSE)
# train_logs <- read_csv('../input/linking-writing-processes-to-writing-quality/train_logs.csv', show_col_types = FALSE)
# sample_submission <- read_csv('../input/linking-writing-processes-to-writing-quality/sample_submission.csv', show_col_types = FALSE)

train_scores <- read_csv('train_scores.csv', show_col_types = FALSE)
train_logs <- read_csv('train_logs.csv', show_col_types = FALSE)
test_logs <- read_csv('test_logs.csv', show_col_types = FALSE)
sample_submission <- read_csv('sample_submission.csv', show_col_types = FALSE)

```

## Feature engineering

### Functions

```{r}
# lists
pause_threshold <- 2000
burst_threshold <- 1000
activities <- c('Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste')
events <- c('q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',',
            'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', "'", 'Delete', 'Unidentified')
text_changes <- c('q', 'NoChange', '\n')
punctuation <- c('\\"', '\\.', '\\,', '\\\'', '\\-', '\\;', '\\:', '\\?', '\\!', '\\<', '\\>', '\\/', '\\#', '\\$', '\\%', '\\^', '\\&', '\\*', '\\(', '\\)', '\\_', '\\+')
lags <- c(25, 50, 100)
lag_cols <- c("action_time_gap25", "action_time_gap50", "action_time_gap100")

# Function to calculate activity counts
activity_counts <- function(df) {
  tmp_df <- df %>%
    filter(activity %in% activities) %>% 
    group_by(id) %>%
    summarise(activity = list(activity)) %>%
    unnest(activity) %>%
    count(id, activity) %>%
    complete(id, activity = activities, fill = list(n = 0)) %>%
    spread(activity, n, fill = 0) %>%
    rename_with(~paste0('activity_', .), -id)

  return(tmp_df)
}

# Function to calculate event counts
event_counts <- function(df) {
  tmp_df <- df %>%
    filter(down_event %in% events) %>% 
    group_by(id) %>%
    summarise(down_event = list(down_event)) %>%
    unnest(down_event) %>%
    count(id, down_event) %>%
    complete(id, down_event = events, fill = list(n = 0)) %>%
    spread(down_event, n, fill = 0) %>%
    rename_with(~paste0('event_', .), -id)

  return(tmp_df)
}

# Function to calculate text change counts
text_change_counts <- function(df) {
  tmp_df <- df %>%
    filter(text_change %in% text_changes) %>% 
    group_by(id) %>%
    summarise(text_change = list(text_change)) %>%
    unnest(text_change) %>%
    count(id, text_change) %>%
    complete(id, text_change = text_changes, fill = list(n = 0)) %>%
    spread(text_change, n, fill = 0) %>%
    rename_with(~paste0('text_change_', .), -id)

  return(tmp_df)
}

# Function to get input words
input_words <- function(df) {
  tmp_df <- df %>%
    filter(!str_detect(text_change, '=>'), text_change != 'NoChange') %>%
    group_by(id) %>%
    summarise(text_change = list(text_change)) %>%
    mutate(text_change = map_chr(text_change, ~paste(.x, collapse = ""))) %>%
    mutate(
      input_word_count = map_int(text_change, ~sum(str_count(.x, 'q+')))
    )
  
tmp_df$input_word_length_mean <- sapply(tmp_df$text_change, function(x) {
  if (length(x) > 0) {
    word_lengths <- unlist(map(punctuation, ~str_count(x, .x)))
    return(mean(word_lengths))
  } else {
    return(0)
  }
})

tmp_df <- tmp_df %>% 
  select(-text_change)
  return(tmp_df)
}

# Function to make time lags
time_lags <- function(df, lags) {
  # Initialize features dataframe
  unique_ids <- unique(df$id)
  feats <- data.frame(id = unique_ids)
  
  # Engineering time data
  for (gap in lags) {
    #cat(paste("> for gap", gap, "\n"))
    df[paste("up_time_shift", gap, sep = "")] <- ave(df$up_time, df$id, FUN = function(x) c(rep(NA, gap), head(x, -gap)))
    df[paste("action_time_gap", gap, sep = "")] <- df$down_time - df[paste("up_time_shift", gap, sep = "")]
  }
  df <- df[, -grep("up_time_shift", names(df))]
  return(df)
}


```

### Features

```{r}

train_logs <- time_lags(train_logs, lags)
Missing <- setdiff(lag_cols, names(train_logs))  # Find names of missing columns (mostly due to test data)
train_logs[Missing] <- 0                    # Add them, filled with '0's (mostly due to test data)



train_logs_1 <- activity_counts(train_logs)
train_logs_2 <- event_counts(train_logs)
train_logs_3 <- text_change_counts(train_logs)
train_logs_4 <- input_words(train_logs)

train_logs_adj <- train_logs %>% 
  mutate(IKI = down_time - lag(up_time, default = 0),
         pause = if_else(IKI >= pause_threshold, 1, 0),
         is_char = ifelse(text_change != "NoChange",str_length(text_change),0),
         is_char = ifelse(is.na(is_char),1,is_char),
         p = if_else(IKI <= burst_threshold & activity != "Nonproduction", 1, 0),
         p_burst = cumsum(p)+1) %>% 
  group_by(p_burst) %>% 
  mutate(p_burst_length = sum(activity == "Input")) %>% 
  ungroup() %>% 
  left_join(train_logs_1,
            join_by(id),
            keep = FALSE) %>% 
  left_join(train_logs_2,
            join_by(id),
            keep = FALSE) %>% 
  left_join(train_logs_3,
            join_by(id),
            keep = FALSE) %>% 
  left_join(train_logs_4,
            join_by(id),
            keep = FALSE)



train_df <- train_logs_adj %>%  
  group_by(id) %>% 
  summarize(
    last_pos = max(cursor_position),
    word_count = max(word_count),
    net_time = round(sum(action_time)/1000,0),
    time = round(max(up_time)/1000,0),
    first_time = round(min(down_time)/1000,0),
    pauses = sum(pause),
    pauses_share = round(pauses/time,3),
    typing_speed = round(word_count/time,3),
    p_burst = n_distinct(p_burst),
    p_burst_per_min = round(p_burst/(net_time/60),0),
    p_burst_length = round(mean(p_burst_length),0),
    action_time_gap25_mean = mean(action_time_gap25, na.rm = TRUE),
    action_time_gap25_min = min(action_time_gap25, na.rm = TRUE),
    action_time_gap25_max = max(action_time_gap25, na.rm = TRUE),
    action_time_gap50_mean = mean(action_time_gap50, na.rm = TRUE),
    action_time_gap50_min = min(action_time_gap50, na.rm = TRUE),
    action_time_gap50_max = max(action_time_gap50, na.rm = TRUE),
    action_time_gap50_mean = mean(action_time_gap50, na.rm = TRUE),
    action_time_gap100_min = min(action_time_gap100, na.rm = TRUE),
    action_time_gap100_max = max(action_time_gap100, na.rm = TRUE),
    action_time_gap100_mean = mean(action_time_gap100, na.rm = TRUE),
    activity_Input = mean(activity_Input , na.rm = TRUE),
    activity_Nonproduction = mean(activity_Nonproduction , na.rm = TRUE),
    activity_Paste = mean(activity_Paste , na.rm = TRUE),
    `activity_Remove/Cut` = mean(`activity_Remove/Cut` , na.rm = TRUE),
    activity_Replace = mean(activity_Replace , na.rm = TRUE),
    `event_'` = mean(`event_'` , na.rm = TRUE),
    `event_,` = mean(`event_,`, na.rm = TRUE),
    event_. = mean(event_., na.rm = TRUE),
    event_ArrowDown = mean(event_ArrowDown , na.rm = TRUE),
    event_ArrowLeft = mean(event_ArrowLeft , na.rm = TRUE),
    event_ArrowRight = mean(event_ArrowRight , na.rm = TRUE),
    event_ArrowUp = mean(event_ArrowUp , na.rm = TRUE),
    event_Backspace = mean(event_Backspace , na.rm = TRUE),
    event_CapsLock = mean(event_CapsLock , na.rm = TRUE),
    event_Delete = mean(event_Delete , na.rm = TRUE),
    event_Enter = mean(event_Enter , na.rm = TRUE),
    event_Leftclick = mean(event_Leftclick , na.rm = TRUE),
    event_q = mean(event_q , na.rm = TRUE),
    event_Shift = mean(event_Shift , na.rm = TRUE),
    event_Space = mean(event_Space , na.rm = TRUE),
    event_Unidentified = mean(event_Unidentified , na.rm = TRUE),
    `text_change_\n` = mean(`text_change_
` , na.rm = TRUE),
    text_change_NoChange = mean(text_change_NoChange , na.rm = TRUE),
    text_change_q = mean(text_change_q , na.rm = TRUE),
    input_word_count = mean(input_word_count , na.rm = TRUE),
    input_word_length_mean = mean(input_word_length_mean , na.rm = TRUE),
input_word_length_min = min(input_word_length_mean , na.rm = TRUE),
input_word_length_man = max(input_word_length_mean , na.rm = TRUE),
input_word_length_std = sd(input_word_length_mean , na.rm = TRUE),
    chars_per_min = round((last_pos - event_Space)/(net_time/60),0),
    accuracy = round((activity_Input - activity_Replace -  `activity_Remove/Cut`)/activity_Input,3)
    ) %>% 
  left_join(train_scores,
            join_by(id),
            keep = FALSE) %>% 
  filter(chars_per_min > 0)
```

### Summary of train dataframe

```{r}
summary(train_df)
```

## Model preparation

### Specification

```{r}
set.seed(123)
train_split <- initial_split(train_df %>% select(-id) , prop = 0.7, strata = score)
train_data <- training(train_split)
test_data <- testing(train_split)

train_folds <- vfold_cv(train_data, v = 5)

model_rec <- recipe(score~., data = train_data) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_pca()
  #step_corr(all_predictors(), threshold = 0.9) # removing correlated features as redundant

xgb_spec <- boost_tree(trees = 1000, 
                       tree_depth = tune(), ## model complexity
                       min_n = tune(), ## model complexity
                       loss_reduction = tune(), ## model complexity
                       sample_size = tune(), mtry = tune(), ## randomness
                       learn_rate = tune() ## step size
                       ) %>%
        set_engine("xgboost") %>% 
        set_mode("regression")

xgb_wf <- workflow() %>% 
        add_recipe(model_rec) %>%
        add_model(xgb_spec)

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), train_data),
  learn_rate(),
  size = 30
)
```

### Grid tuning with tidymodels

```{r}
registerDoParallel(detectCores() - 2)

tic()
set.seed(1308)


xgb_res <-
  tune_grid(
    xgb_wf,
    resamples = train_folds,
    grid = xgb_grid,
    control = control_grid(save_pred = TRUE)
  )

toc()

unregister_dopar()

collect_metrics(xgb_res)
```

```{r}
# results
p <- xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "rmse")

ggplotly(p)
```

```{r eval=FALSE, include=FALSE}
# feature importance

?? %>%
  fit(data = train_data) %>%
  pull_workflow_fit() %>%
  vip(geom = "point", 
      num_features = 20)
```

```{r}
best_results <- 
        select_best(xgb_res, metric = "rmse")

best_results

xg_fit_rs <- 
   xgb_wf %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = train_split)

rmse_xg <- xg_fit_rs %>% collect_metrics()

rmse_xg$.estimate[1]

```

### Tuning with Python optuna

```{r}
# wrap the tidymodel that we created above into a function. rlang !! injection operator to define the variables. This is because NULL values result in an error due to delayed evaluation of the variables by the boost_tree() function.

tune_r_model <- function(trees = 1000, tree_depth = NULL, min_n = NULL, loss_reduction = NULL, sample_size = NULL, mtry = NULL, learn_rate = NULL){
   
  set.seed(123)
  xg_model <-
  boost_tree(
    trees = !!trees, 
    tree_depth = !!tree_depth, 
    min_n = !!min_n, 
    loss_reduction = !!loss_reduction,            
    sample_size = !!sample_size, 
    mtry = !!mtry,         
    learn_rate = !!learn_rate  
  ) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
  
  #Recipe
  xg_recipe <-
    recipe(score~., data = train_data) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_pca()

  #Workflow
  xg_workflow <-
    workflow() %>%
    add_recipe(xg_recipe) %>%
    add_model(xg_model)
  
xg_fit_rs <- 
   xg_workflow %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = train_split)
  
  rmse_xg <- xg_fit_rs %>% collect_metrics()
  rmse_xg <- rmse_xg$.estimate[1]
  return(rmse_xg)
}

grid_size <- 30
```

```{r eval=FALSE, include=FALSE}
# Create a conda environment
conda_create("my_conda_env", packages = c("python=3.12"))

# Use the conda environment
use_condaenv("my_conda_env")

conda_install("my_conda_env", "xgboost")


```

```{python}
# Using the reticulate package we can run Python code directly in Rstudio and call the tune_R_model function that we have defined above simply by adding an r. in front of the functions name.


import optuna

from optuna.samplers import TPESampler

import math

def objective(trial):
    tree_depth = trial.suggest_int('tree_depth',1, 15)
    min_n = trial.suggest_int('min_n',2, 15)
    loss_reduction = trial.suggest_float("loss_reduction", 1e-3, 1e-2, log=True)
    sample_size = trial.suggest_float("sample_size", 0.1, 0.5)
    mtry = trial.suggest_int("mtry", 25, 50)
    learn_rate = trial.suggest_float("learn_rate", 1e-3, 1e-2, log=True)
    
    out = r.tune_r_model(
      trees = 1000,
      tree_depth = tree_depth,
      min_n = min_n, 
      loss_reduction = loss_reduction,            
      sample_size = sample_size, 
      mtry = mtry,         
      learn_rate = learn_rate  
      )
      
    return out

study = optuna.create_study(
  direction="minimize", 
  sampler=TPESampler(seed=123) #For reproducible results
  )
  
study.optimize(objective, n_trials=100)

print("The best found Accuracy = {}".format(round(study.best_value,3)))
print("With Parameters: {}".format(study.best_params))
```


```{python}
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings("ignore")

def objective(trial):
    params = {
    'n_estimators' : trial.suggest_int('n_estimators',500,2000),
    'max_depth':  trial.suggest_int('max_depth',3,8),
    'min_child_weight': trial.suggest_float('min_child_weight', 2,4),
    "learning_rate" : trial.suggest_float('learning_rate',1e-4, 0.2),
    'subsample': trial.suggest_float('subsample', 0.2, 1),
    'gamma': trial.suggest_float("gamma", 1e-4, 1.0),
    "colsample_bytree" : trial.suggest_float('colsample_bytree',0.2,1),
    "colsample_bylevel" : trial.suggest_float('colsample_bylevel',0.2,1),
    "colsample_bynode" : trial.suggest_float('colsample_bynode',0.2,1),
    }

    xgbmodel_optuna = XGBRegressor(**params,random_state=seed,tree_method = "gpu_hist")
    xgbmodel_optuna.fit(X,y)
    cv = -1*cross_val_score(xgbmodel_optuna, X, y, cv = 4,scoring='neg_root_mean_squared_error').mean()
    return cv

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100,timeout=5000)

# The rest of your Python code

train_data.drop = r.train
# Split your data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(train_data.drop(columns='score'), train_data['score'], test_size=0.2, random_state=123)

study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=123))

study.optimize(objective, n_trials=100)

print("The best found RMSE =", round(study.best_value, 3))
print("With Parameters:", study.best_params)

```

### Predicting

```{r}
test_logs <- time_lags(test_logs, lags)
Missing <- setdiff(lag_cols, names(test_logs))  # Find names of missing columns (mostly due to test data)
test_logs[Missing] <- 0                    # Add them, filled with '0's (mostly due to test data)

test_logs_1 <- activity_counts(test_logs)
test_logs_2 <- event_counts(test_logs)
test_logs_3 <- text_change_counts(test_logs)
test_logs_4 <- input_words(test_logs)

test_logs_adj <- test_logs %>% 
  mutate(IKI = down_time - lag(up_time, default = 0),
         pause = if_else(IKI >= pause_threshold, 1, 0),
         is_char = ifelse(text_change != "NoChange",str_length(text_change),0),
         is_char = ifelse(is.na(is_char),1,is_char),
         p = if_else(IKI <= burst_threshold & activity != "Nonproduction", 1, 0),
         p_burst = cumsum(p)+1) %>% 
  group_by(p_burst) %>% 
  mutate(p_burst_length = sum(activity == "Input")) %>% 
  ungroup() %>% 
  left_join(test_logs_1,
            join_by(id),
            keep = FALSE) %>% 
  left_join(test_logs_2,
            join_by(id),
            keep = FALSE) %>% 
  left_join(test_logs_3,
            join_by(id),
            keep = FALSE) %>% 
  left_join(test_logs_4,
            join_by(id),
            keep = FALSE)


colnames(test_logs_adj)

  
  
test_df <- test_logs_adj %>%  
  group_by(id) %>% 
  summarize(
    last_pos = max(cursor_position),
    word_count = max(word_count),
    net_time = round(sum(action_time)/1000,0),
    time = round(max(up_time)/1000,0),
    first_time = round(min(down_time)/1000,0),
    pauses = sum(pause),
    pauses_share = round(pauses/time,3),
    typing_speed = round(word_count/time,3),
    p_burst = n_distinct(p_burst),
    p_burst_per_min = round(p_burst/(net_time/60),0),
    p_burst_length = round(mean(p_burst_length),0),
    action_time_gap25_mean = mean(action_time_gap25, na.rm = TRUE),
    action_time_gap25_min = min(action_time_gap25, na.rm = TRUE),
    action_time_gap25_max = max(action_time_gap25, na.rm = TRUE),
    action_time_gap50_mean = mean(action_time_gap50, na.rm = TRUE),
    action_time_gap50_min = min(action_time_gap50, na.rm = TRUE),
    action_time_gap50_max = max(action_time_gap50, na.rm = TRUE),
    action_time_gap50_mean = mean(action_time_gap50, na.rm = TRUE),
    action_time_gap100_min = min(action_time_gap100, na.rm = TRUE),
    action_time_gap100_max = max(action_time_gap100, na.rm = TRUE),
    action_time_gap100_mean = mean(action_time_gap100, na.rm = TRUE),
    activity_Input = mean(activity_Input , na.rm = TRUE),
    activity_Nonproduction = mean(activity_Nonproduction , na.rm = TRUE),
    activity_Paste = mean(activity_Paste , na.rm = TRUE),
    `activity_Remove/Cut` = mean(`activity_Remove/Cut` , na.rm = TRUE),
    activity_Replace = mean(activity_Replace , na.rm = TRUE),
    `event_'` = mean(`event_'` , na.rm = TRUE),
    `event_,` = mean(`event_,`, na.rm = TRUE),
    event_. = mean(event_., na.rm = TRUE),
    event_ArrowDown = mean(event_ArrowDown , na.rm = TRUE),
    event_ArrowLeft = mean(event_ArrowLeft , na.rm = TRUE),
    event_ArrowRight = mean(event_ArrowRight , na.rm = TRUE),
    event_ArrowUp = mean(event_ArrowUp , na.rm = TRUE),
    event_Backspace = mean(event_Backspace , na.rm = TRUE),
    event_CapsLock = mean(event_CapsLock , na.rm = TRUE),
    event_Delete = mean(event_Delete , na.rm = TRUE),
    event_Enter = mean(event_Enter , na.rm = TRUE),
    event_Leftclick = mean(event_Leftclick , na.rm = TRUE),
    event_q = mean(event_q , na.rm = TRUE),
    event_Shift = mean(event_Shift , na.rm = TRUE),
    event_Space = mean(event_Space , na.rm = TRUE),
    event_Unidentified = mean(event_Unidentified , na.rm = TRUE),
    `text_change_\n` = mean(`text_change_
` , na.rm = TRUE),
    text_change_NoChange = mean(text_change_NoChange , na.rm = TRUE),
    text_change_q = mean(text_change_q , na.rm = TRUE),
    input_word_count = mean(input_word_count , na.rm = TRUE),
    input_word_length_mean = mean(input_word_length_mean , na.rm = TRUE),
input_word_length_min = min(input_word_length_mean , na.rm = TRUE),
input_word_length_man = max(input_word_length_mean , na.rm = TRUE),
input_word_length_std = sd(input_word_length_mean , na.rm = TRUE),
    chars_per_min = round((last_pos - event_Space)/(net_time/60),0),
    accuracy = round((activity_Input - activity_Replace -  `activity_Remove/Cut`)/activity_Input,3)
    ) %>% 
  filter(chars_per_min > 0)


  test_df <- test_df %>% mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x))
  test_df[is.na(test_df)] <- 0
```

```{r eval=FALSE, include=FALSE}
pred_test <- predict(model_fit$.workflow[[1]], new_data = test_df)

sample_submission$score <- pred_test$.pred
write_csv(sample_submission, "submission.csv")
```

```{r}
fitted_xgb <- 
   boosting_final$.workflow[[1]] %>% 
   fit(data = train_df)

pred_test <- predict(fitted_xgb, new_data = test_df)

sample_submission$score <- pred_test$.pred
write_csv(sample_submission, "submission.csv")
```
