---
title: ''
author: "Arnold Kakas"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(
  message = TRUE,
  warning = FALSE
)
```

<https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline> 
<br><https://www.kaggle.com/code/pehahn/xgb-base-in-r> 
<br><https://juliasilge.com/blog/xgboost-tune-volleyball/>
<br><https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#specify-models>
<br><https://www.youtube.com/watch?v=44rINyxp220>
<br><https://www.analyticsvidhya.com/blog/2021/08/ensemble-stacking-for-machine-learning-and-deep-learning/>

## Load packages and data

```{r}
library(tidyverse)
  library(bonsai) # lightgbm
  library(kknn) # knn engine
  library(janitor)
library(randomForest)
  library(tidymodels)
library(rlang)
print("Done")
```


```{r}
# train_scores <- read_csv('../input/linking-writing-processes-to-writing-quality/train_scores.csv', show_col_types = FALSE)
# test_logs <- read_csv('../input/linking-writing-processes-to-writing-quality/test_logs.csv', show_col_types = FALSE)
# train_logs <- read_csv('../input/linking-writing-processes-to-writing-quality/train_logs.csv', show_col_types = FALSE)
# sample_submission <- read_csv('../input/linking-writing-processes-to-writing-quality/sample_submission.csv', show_col_types = FALSE)

train_scores <- read_csv("train_scores.csv", show_col_types = FALSE)
train_logs <- read_csv("train_logs.csv", show_col_types = FALSE)
test_logs <- read_csv("test_logs.csv", show_col_types = FALSE)
sample_submission <- read_csv("sample_submission.csv", show_col_types = FALSE)
```

## Feature engineering

### Functions

```{r}
# lists
pause_threshold <- 2000
burst_threshold <- 1000
activities <- c("Input", "Remove/Cut", "Nonproduction", "Replace", "Paste")
events <- c(
  "q", "Space", "Backspace", "Shift", "ArrowRight", "Leftclick", "ArrowLeft", ".", ",",
  "ArrowDown", "ArrowUp", "Enter", "CapsLock", "'", "Delete", "Unidentified"
)
text_changes <- c("q", "NoChange", "\n")
punctuation <- c('\\"', "\\.", "\\,", "\\'", "\\-", "\\;", "\\:", "\\?", "\\!", "\\<", "\\>", "\\/", "\\#", "\\$", "\\%", "\\^", "\\&", "\\*", "\\(", "\\)", "\\_", "\\+")
lags <- c(25, 50, 100)
lag_cols <- c("action_time_gap25", "action_time_gap50", "action_time_gap100")

# Function to calculate activity counts
activity_counts <- function(df) {
  tmp_df <- df %>%
    filter(activity %in% activities) %>%
    group_by(id) %>%
    summarise(activity = list(activity)) %>%
    unnest(activity) %>%
    count(id, activity) %>%
    complete(id, activity = activities, fill = list(n = 0)) %>%
    spread(activity, n, fill = 0) %>%
    rename_with(~ paste0("activity_", .), -id)

  return(tmp_df)
}

# Function to calculate event counts
event_counts <- function(df) {
  tmp_df <- df %>%
    filter(down_event %in% events) %>%
    group_by(id) %>%
    summarise(down_event = list(down_event)) %>%
    unnest(down_event) %>%
    count(id, down_event) %>%
    complete(id, down_event = events, fill = list(n = 0)) %>%
    spread(down_event, n, fill = 0) %>%
    rename_with(~ paste0("event_", .), -id)

  return(tmp_df)
}

# Function to calculate text change counts
text_change_counts <- function(df) {
  tmp_df <- df %>%
    filter(text_change %in% text_changes) %>%
    group_by(id) %>%
    summarise(text_change = list(text_change)) %>%
    unnest(text_change) %>%
    count(id, text_change) %>%
    complete(id, text_change = text_changes, fill = list(n = 0)) %>%
    spread(text_change, n, fill = 0) %>%
    rename_with(~ paste0("text_change_", .), -id)

  return(tmp_df)
}

# Function to get input words
# Function to get input words
input_words <- function(df) {
  tmp_df <- df %>%
    filter(!str_detect(text_change, "=>"), text_change != "NoChange") %>%
    group_by(id) %>%
    summarise(text_change = list(text_change)) %>%
    mutate(text_change = map_chr(text_change, ~ paste(.x, collapse = ""))) %>%
    mutate(
      input_word_count = map_int(text_change, ~ sum(str_count(.x, "q+")))
    ) %>%
    mutate(
      input_word_length_mean = sapply(text_change, function(x) {
        if (length(x) > 0) {
          word_lengths <- unlist(map(punctuation, ~ str_count(x, .x)))
          # Filter word_lengths greater than 0
          word_lengths <- word_lengths[word_lengths > 0]
          if (length(word_lengths) > 0) {
            return(mean(word_lengths))
          } else {
            return(0)
          }
        } else {
          return(0)
        }
      })
    ) %>%
    mutate(
      input_word_length_geometric_mean = sapply(text_change, function(x) {
        if (length(x) > 0) {
          word_lengths <- unlist(map(punctuation, ~ str_count(x, .x)))
          # Filter word_lengths greater than 0
          word_lengths <- word_lengths[word_lengths > 0]
          if (length(word_lengths) > 0) {
            return(exp(mean(log(word_lengths))))
          } else {
            return(0)
          }
        } else {
          return(0)
        }
      })
    ) %>%
    mutate(
      input_word_length_iqr = sapply(text_change, function(x) {
        if (length(x) > 0) {
          word_lengths <- unlist(map(punctuation, ~ str_count(x, .x)))
          # Filter word_lengths greater than 0
          word_lengths <- word_lengths[word_lengths > 0]
          if (length(word_lengths) > 0) {
            return(IQR(word_lengths))
          } else {
            return(0)
          }
        } else {
          return(0)
        }
      })
    ) %>%
    mutate(
      input_word_length_sd = sapply(text_change, function(x) {
        if (length(x) > 0) {
          word_lengths <- unlist(map(punctuation, ~ str_count(x, .x)))
          # Filter word_lengths greater than 0
          word_lengths <- word_lengths[word_lengths > 0]
          if (length(word_lengths) > 0) {
            return(sd(word_lengths))
          } else {
            return(0)
          }
        } else {
          return(0)
        }
      })
    )



  tmp_df <- tmp_df %>%
    select(-text_change)
  return(tmp_df)

  return(tmp_df)
}




# Function to make time lags
# time_lags <- function(df, lags) {
#   # Initialize features dataframe
#   unique_ids <- unique(df$id)
#   feats <- data.frame(id = unique_ids)
# 
#   # Engineering time data
#   for (gap in lags) {
#     # cat(paste("> for gap", gap, "\n"))
#     df[paste("up_time_shift", gap, sep = "")] <- ave(df$up_time, df$id, FUN = function(x) c(rep(NA, gap), head(x, -gap)))
#     df[paste("action_time_gap", gap, sep = "")] <- df$down_time - df[paste("up_time_shift", gap, sep = "")]
#   }
#   df <- df[, -grep("up_time_shift", names(df))]
#   return(df)
# }


time_lags <- function(df, lags) {
  # Initialize features dataframe
  unique_ids <- unique(df$id)
  feats <- data.frame(id = unique_ids)

  for (gap in lags) {
    col_name_shift <- sym(paste("up_time_shift", gap, sep = ""))
    col_name_gap <- sym(paste("action_time_gap", gap, sep = ""))

    df <- df %>%
      group_by(id) %>%
      mutate(
        !!col_name_shift := dplyr::lag(up_time, n = gap),
        !!col_name_gap := down_time - !!col_name_shift
      )
  }

  # Remove the columns with "up_time_shift" in their names
  df <- df %>%
    select(-matches("up_time_shift"))

  # Remove grouping
  df <- df %>% ungroup()

  return(df)
}

```

### Features


```{r}
train_logs_adj <- time_lags(train_logs, lags)

# List of missing column names
train_missing_cols <- setdiff(lag_cols, names(train_logs_adj))

# Loop to add missing columns and fill with zeros
for (col_name in train_missing_cols) {
  train_logs_adj[[col_name]] <- 0
}


train_logs_1 <- activity_counts(train_logs)
train_logs_2 <- event_counts(train_logs)
train_logs_3 <- text_change_counts(train_logs)
train_logs_4 <- input_words(train_logs)

train_logs_adj <- train_logs_adj %>%
  group_by(id) %>%
  mutate(
    IKI = down_time - dplyr::lag(down_time, default = 1),
    action_time_std = sd(action_time, na.rm = TRUE),
    pause = if_else(IKI >= pause_threshold, 1, 0),
    is_char = ifelse(text_change != "NoChange", str_length(text_change), 0),
    is_char = ifelse(is.na(is_char), 1, is_char),
    p = if_else(IKI <= burst_threshold & activity != "Nonproduction", 1, 0),
    p_burst = cumsum(p) + 1
  ) %>%
  ungroup() %>%
  group_by(p_burst) %>%
  mutate(p_burst_length = sum(activity == "Input")) %>%
  ungroup() %>%
  left_join(train_logs_1,
    join_by(id),
    keep = FALSE
  ) %>%
  left_join(train_logs_2,
    join_by(id),
    keep = FALSE
  ) %>%
  left_join(train_logs_3,
    join_by(id),
    keep = FALSE
  ) %>%
  left_join(train_logs_4,
    join_by(id),
    keep = FALSE
  ) %>%
  mutate(across(where(is.numeric), ~ replace_na(.x, 1)), # due to lag columns
    IKI = if_else(IKI <= 0, 1, IKI),
    action_time_gap25 = if_else(action_time_gap25 <= 0, 1, action_time_gap25),
    action_time_gap50 = if_else(action_time_gap50 <= 0, 1, action_time_gap50),
    action_time_gap100 = if_else(action_time_gap100 <= 0, 1, action_time_gap100)
  )


train_df <- train_logs_adj %>%
  group_by(id) %>%
  summarize(
    last_pos = max(cursor_position),
    total_events = max(event_id),
    word_count = max(word_count),
    net_time = round(sum(action_time) / 1000, 0),
    time = round(max(up_time) / 1000, 0),
    first_time = round(min(down_time) / 1000, 0),
    pauses = sum(pause),
    pauses_share = round(pauses / time, 3),
    IKI_geometric_mean = exp(mean(log(IKI))),
    IKI_IQR = IQR(IKI, na.rm = TRUE),
    IKI_max = max(IKI, na.rm = TRUE),
    typing_speed = round(word_count / time, 3),
    p_burst = n_distinct(p_burst),
    p_burst_per_min = round(p_burst / (net_time / 60), 0),
    p_burst_length = round(mean(p_burst_length), 0),
    action_time_gap25_mean = mean(action_time_gap25, na.rm = TRUE),
    action_time_gap25_geometric_mean = exp(mean(log(action_time_gap25))),
    action_time_gap25_max = max(action_time_gap25, na.rm = TRUE),
    action_time_gap50_mean = mean(action_time_gap50, na.rm = TRUE),
    action_time_gap50_geometric_mean = exp(mean(log(action_time_gap50))),
    action_time_gap50_max = max(action_time_gap50, na.rm = TRUE),
    action_time_gap100_max = max(action_time_gap100, na.rm = TRUE),
    action_time_gap100_mean = mean(action_time_gap100, na.rm = TRUE),
    action_time_gap100_geometric_mean = exp(mean(log(action_time_gap100))),
    activity_Input = mean(activity_Input, na.rm = TRUE),
    activity_Nonproduction = mean(activity_Nonproduction, na.rm = TRUE),
    activity_Paste = mean(activity_Paste, na.rm = TRUE),
    `activity_Remove/Cut` = mean(`activity_Remove/Cut`, na.rm = TRUE),
    activity_Replace = mean(activity_Replace, na.rm = TRUE),
    `event_'` = mean(`event_'`, na.rm = TRUE),
    `event_,` = mean(`event_,`, na.rm = TRUE),
    event_. = mean(event_., na.rm = TRUE),
    event_ArrowDown = mean(event_ArrowDown, na.rm = TRUE),
    event_ArrowLeft = mean(event_ArrowLeft, na.rm = TRUE),
    event_ArrowRight = mean(event_ArrowRight, na.rm = TRUE),
    event_ArrowUp = mean(event_ArrowUp, na.rm = TRUE),
    event_Backspace = mean(event_Backspace, na.rm = TRUE),
    event_CapsLock = mean(event_CapsLock, na.rm = TRUE),
    event_Delete = mean(event_Delete, na.rm = TRUE),
    event_Enter = mean(event_Enter, na.rm = TRUE),
    event_Leftclick = mean(event_Leftclick, na.rm = TRUE),
    event_q = mean(event_q, na.rm = TRUE),
    event_Shift = mean(event_Shift, na.rm = TRUE),
    event_Space = mean(event_Space, na.rm = TRUE),
    event_Unidentified = mean(event_Unidentified, na.rm = TRUE),
    `text_change_\n` = mean(`text_change_
`, na.rm = TRUE),
    text_change_NoChange = mean(text_change_NoChange, na.rm = TRUE),
    text_change_q = mean(text_change_q, na.rm = TRUE),
    input_word_count = mean(input_word_count, na.rm = TRUE),
    input_word_length_mean = mean(input_word_length_mean, na.rm = TRUE),
    input_word_length_geometric_mean = mean(input_word_length_geometric_mean, na.rm = TRUE),
    accuracy = round((activity_Input - activity_Replace - `activity_Remove/Cut`) / activity_Input, 3),
    words_per_sec = input_word_count / time,
    words_per_event = input_word_count / total_events,
    events_per_sec = total_events / time
  ) %>%
  left_join(train_scores,
    join_by(id),
    keep = FALSE
  )
```


### Select and adjust features based on EDA

```{r}
train_df_reduced <- train_df %>%
  select(-c(
    accuracy,
    input_word_length_geometric_mean,
    pauses_share,
    activity_Paste,
    event_Unidentified,
    event_ArrowUp,
    event_Delete,
    event_Enter,
    event_Leftclick
  ))
```

```{r}
# Function to remove outliers
remove_outliers <- function(df) {
  # Get a list of column names where we need to eliminate outliers, based on distributions
  cols_to_check <- c(
    "first_time",
    "IKI_max",
    "event_ArrowDown",
    "event_ArrowLeft",
    "event_ArrowRight",
    "event_CapsLock",
    "words_per_sec"
  )

  # Calculate outlier margins for each column
  margins <- lapply(cols_to_check, function(col) {
    col_mean <- mean(df[[col]], na.rm = TRUE)
    col_iqr <- IQR(df[[col]], na.rm = TRUE)
    # lower_margin <- col_mean - 1.5 * col_iqr
    upper_margin <- col_mean + 1.5 * col_iqr

    return(list(column = col, upper_margin = upper_margin)) # lower_margin = lower_margin,
  })

  # Select and apply filtering conditions for the first n elements in 'margins' list
  n <- length(margins) # Use the length of selected_margins

  # Filter the DataFrame based on the selected margins using rlang
  filtered_df <- df
  for (i in 1:n) {
    col <- margins[[i]]$column
    # lower_margin <- margins[[i]]$lower_margin
    upper_margin <- margins[[i]]$upper_margin

    # Use rlang to reference columns dynamically
    filtered_df <- filtered_df %>%
      filter(!!sym(col) <= upper_margin) # !!sym(col) >= lower_margin,
  }

  return(filtered_df)
}
```

```{r}
# Call the function to filter your 'train_df' dataframe
filtered_train_df <- remove_outliers(train_df_reduced)
```

### Select features 2
```{r}
train_df_reduced_2 <- filtered_train_df %>%
  select(-c(
    time,
    event_ArrowDown,
    event_ArrowLeft,
    event_ArrowRight,
    event_CapsLock,
    `text_change_
`,
    p_burst_per_min
  ))
```


```{r}
train_df_reduced_2 <- train_df_reduced_2 %>% clean_names()
```


```{r}
# Function to scale numeric columns in a dataframe, excluding "id"
scale_numeric_columns <- function(df) {
  # Identify numeric columns except "id"
  numeric_columns <- setdiff(names(df), c("id", "score"))
  
  # Scale numeric columns
  df[, numeric_columns] <- scale(df[, numeric_columns])
  
  return(df)
}


scaled_train_df <- scale_numeric_columns(train_df_reduced_2)
```


## Models preparation

### Specifications


```{r}
# Recipe for predictions
prediciton_model_rec <- recipe(score ~ ., data = scaled_train_df %>% select (-id))

prediciton_model_rec_glm <- recipe(score ~ ., data = scaled_train_df %>% select (-id)) %>%
  step_naomit(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors())
```


```{r}
# xgboost
xgb_spec <- boost_tree(
  trees = 673, tree_depth = 1, min_n = 12, loss_reduction = 6.368522e-09, ## model complexity
  sample_size = 0.712659, mtry = 23, ## randomness
  learn_rate = 0.05083097 ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_wf <- workflow() %>%
  add_recipe(prediciton_model_rec) %>%
  add_model(xgb_spec) %>% 
  fit(scaled_train_df %>% select (-id))
```

```{r}
# lightgbm
lgbm_spec <- boost_tree(
  trees = 230, tree_depth = 5, min_n = 5,
  loss_reduction = 	
1.001074, learn_rate = 0.03438409,
  sample_size = 0.817721
) %>%
  set_engine("lightgbm") %>%
  set_mode("regression")


lgbm_wf <- workflow() %>%
  add_recipe(prediciton_model_rec) %>%
  add_model(lgbm_spec) %>% 
  fit(scaled_train_df %>% select (-id))
```

```{r}
# random forest
rf_spec <- rand_forest(
  trees = 205, min_n = 2, ## model complexity
  mtry = 17, ## randomness
) %>%
  set_engine("randomForest") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(prediciton_model_rec) %>%
  add_model(rf_spec) %>% 
  fit(scaled_train_df %>% select (-id))
```

```{r}
# linear regression
lr_spec <- linear_reg(
  penalty = 0.03101148,
  mixture = 0.04702358
) %>%
  set_engine(engine = "glmnet") %>%
  set_mode("regression")

lr_wf <- workflow() %>%
  add_recipe(prediciton_model_rec_glm) %>%
  add_model(lr_spec) %>% 
  fit(scaled_train_df %>% select (-id))
```

```{r}
# knn
knn_spec <- nearest_neighbor(
  neighbors = 10,
  weight_func = "gaussian",
  dist_power = 1.001154
) %>%
  set_engine("kknn") %>%
  set_mode("regression")

knn_wf <- workflow() %>%
  add_recipe(prediciton_model_rec) %>%
  add_model(knn_spec) %>% 
  fit(scaled_train_df %>% select (-id))
```

### Prep test dataset

```{r}
test_logs_adj <- time_lags(test_logs, lags)

# List of missing column names
missing_cols <- setdiff(lag_cols, names(test_logs_adj))

# Loop to add missing columns and fill with zeros
for (col_name in missing_cols) {
  test_logs_adj[[col_name]] <- 0
}

test_logs_1 <- activity_counts(test_logs)
test_logs_2 <- event_counts(test_logs)
test_logs_3 <- text_change_counts(test_logs)
test_logs_4 <- input_words(test_logs)

test_logs_adj <- test_logs_adj %>%
  group_by(id) %>%
  mutate(
    IKI = down_time - dplyr::lag(down_time, default = 1),
    action_time_std = sd(action_time, na.rm = TRUE),
    pause = if_else(IKI >= pause_threshold, 1, 0),
    is_char = ifelse(text_change != "NoChange", str_length(text_change), 0),
    is_char = ifelse(is.na(is_char), 1, is_char),
    p = if_else(IKI <= burst_threshold & activity != "Nonproduction", 1, 0),
    p_burst = cumsum(p) + 1
  ) %>%
  ungroup() %>%
  group_by(p_burst) %>%
  mutate(p_burst_length = sum(activity == "Input")) %>%
  ungroup() %>%
  left_join(test_logs_1,
    join_by(id),
    keep = FALSE
  ) %>%
  left_join(test_logs_2,
    join_by(id),
    keep = FALSE
  ) %>%
  left_join(test_logs_3,
    join_by(id),
    keep = FALSE
  ) %>%
  left_join(test_logs_4,
    join_by(id),
    keep = FALSE
  ) %>%
  mutate(across(where(is.numeric), ~ replace_na(.x, 1)), # due to lag columns
    IKI = if_else(IKI <= 0, 1, IKI),
    action_time_gap25 = if_else(action_time_gap25 <= 0, 1, action_time_gap25),
    action_time_gap50 = if_else(action_time_gap50 <= 0, 1, action_time_gap50),
    action_time_gap100 = if_else(action_time_gap100 <= 0, 1, action_time_gap100)
  )


test_df <- test_logs_adj %>%
  group_by(id) %>%
  summarize(
    last_pos = max(cursor_position),
    total_events = max(event_id),
    word_count = max(word_count),
    net_time = round(sum(action_time) / 1000, 0),
    time = round(max(up_time) / 1000, 0),
    first_time = round(min(down_time) / 1000, 0),
    pauses = sum(pause),
    pauses_share = round(pauses / time, 3),
    IKI_geometric_mean = exp(mean(log(IKI))),
    IKI_IQR = IQR(IKI, na.rm = TRUE),
    IKI_max = max(IKI, na.rm = TRUE),
    typing_speed = round(word_count / time, 3),
    p_burst = n_distinct(p_burst),
    p_burst_per_min = round(p_burst / (net_time / 60), 0),
    p_burst_length = round(mean(p_burst_length), 0),
    action_time_gap25_mean = mean(action_time_gap25, na.rm = TRUE),
    action_time_gap25_geometric_mean = exp(mean(log(action_time_gap25))),
    action_time_gap25_max = max(action_time_gap25, na.rm = TRUE),
    action_time_gap50_mean = mean(action_time_gap50, na.rm = TRUE),
    action_time_gap50_geometric_mean = exp(mean(log(action_time_gap50))),
    action_time_gap50_max = max(action_time_gap50, na.rm = TRUE),
    action_time_gap100_max = max(action_time_gap100, na.rm = TRUE),
    action_time_gap100_mean = mean(action_time_gap100, na.rm = TRUE),
    action_time_gap100_geometric_mean = exp(mean(log(action_time_gap100))),
    activity_Input = mean(activity_Input, na.rm = TRUE),
    activity_Nonproduction = mean(activity_Nonproduction, na.rm = TRUE),
    activity_Paste = mean(activity_Paste, na.rm = TRUE),
    `activity_Remove/Cut` = mean(`activity_Remove/Cut`, na.rm = TRUE),
    activity_Replace = mean(activity_Replace, na.rm = TRUE),
    `event_'` = mean(`event_'`, na.rm = TRUE),
    `event_,` = mean(`event_,`, na.rm = TRUE),
    event_. = mean(event_., na.rm = TRUE),
    event_ArrowDown = mean(event_ArrowDown, na.rm = TRUE),
    event_ArrowLeft = mean(event_ArrowLeft, na.rm = TRUE),
    event_ArrowRight = mean(event_ArrowRight, na.rm = TRUE),
    event_ArrowUp = mean(event_ArrowUp, na.rm = TRUE),
    event_Backspace = mean(event_Backspace, na.rm = TRUE),
    event_CapsLock = mean(event_CapsLock, na.rm = TRUE),
    event_Delete = mean(event_Delete, na.rm = TRUE),
    event_Enter = mean(event_Enter, na.rm = TRUE),
    event_Leftclick = mean(event_Leftclick, na.rm = TRUE),
    event_q = mean(event_q, na.rm = TRUE),
    event_Shift = mean(event_Shift, na.rm = TRUE),
    event_Space = mean(event_Space, na.rm = TRUE),
    event_Unidentified = mean(event_Unidentified, na.rm = TRUE),
    `text_change_\n` = mean(`text_change_
`, na.rm = TRUE),
    text_change_NoChange = mean(text_change_NoChange, na.rm = TRUE),
    text_change_q = mean(text_change_q, na.rm = TRUE),
    input_word_count = mean(input_word_count, na.rm = TRUE),
    input_word_length_mean = mean(input_word_length_mean, na.rm = TRUE),
    input_word_length_geometric_mean = mean(input_word_length_geometric_mean, na.rm = TRUE),
    accuracy = round((activity_Input - activity_Replace - `activity_Remove/Cut`) / activity_Input, 3),
    words_per_sec = input_word_count / time,
    words_per_event = input_word_count / total_events,
    events_per_sec = total_events / time
  )


test_df <- test_df %>% mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x))
test_df[is.na(test_df)] <- 0
```


```{r}
test_df_reduced <- test_df %>%
  select(-c(
    accuracy,
    input_word_length_geometric_mean,
    pauses_share,
    activity_Paste,
    event_Unidentified,
    event_ArrowUp,
    event_Delete,
    event_Enter,
    event_Leftclick
  )
  )
```

```{r}
filtered_test_df <- remove_outliers(test_df_reduced)

filtered_test_df <- filtered_test_df %>% 
  select(-c(
    time,
    event_ArrowDown,
    event_ArrowLeft,
    event_ArrowRight,
    event_CapsLock,
    `text_change_
`,
    p_burst_per_min)
) %>% 
  clean_names()
```

```{r}
scaled_test_df <- scale_numeric_columns(filtered_test_df)

scaled_test_df <- scaled_test_df %>% mutate_if(is.numeric, function(x) ifelse(is.infinite(x), 0, x))
scaled_test_df[is.na(scaled_test_df)] <- 0
```

### Predict with 1st layer models

```{r}
# Predict on train data
train_xgb_pred <- predict(xgb_wf, scaled_train_df %>% select(-id)) %>%
  bind_cols(scaled_train_df)

train_lgbm_pred <- predict(lgbm_wf, scaled_train_df %>% select(-id)) %>%
  bind_cols(scaled_train_df)

train_rf_pred <- predict(rf_wf, scaled_train_df %>% select(-id)) %>%
  bind_cols(scaled_train_df)

train_lr_pred <- predict(lr_wf, scaled_train_df %>% select(-id)) %>%
  bind_cols(scaled_train_df)

train_knn_pred <- predict(knn_wf, scaled_train_df %>% select(-id)) %>%
  bind_cols(scaled_train_df)
```


```{r}
# Predict on test data
xgb_pred <- predict(xgb_wf, scaled_test_df %>% select(-id)) %>%
  bind_cols(scaled_test_df)

lgbm_pred <- predict(lgbm_wf, scaled_test_df %>% select(-id)) %>%
  bind_cols(scaled_test_df)

rf_pred <- predict(rf_wf, scaled_test_df %>% select(-id)) %>%
  bind_cols(scaled_test_df)

lr_pred <- predict(lr_wf, scaled_test_df %>% select(-id)) %>%
  bind_cols(scaled_test_df)

knn_pred <- predict(knn_wf, scaled_test_df %>% select(-id)) %>%
  bind_cols(scaled_test_df)
```


## Stack models and creat Meta Learner

### Prepare stack dataframe(s)

```{r message=FALSE, warning=FALSE}
train_xgb_stack <- train_xgb_pred %>%
  select(id, score, xgb = .pred)

train_lgbm_stack <- train_lgbm_pred %>%
  select(id, lgbm = .pred)

train_rf_stack <- train_rf_pred %>%
  select(id, rf = .pred)

train_lr_stack <- train_lr_pred %>%
  select(id, lr = .pred)

train_knn_stack <- train_knn_pred %>%
  select(id, knn = .pred)

train_stack_data <- train_xgb_stack %>%
  left_join(train_lgbm_stack) %>%
  left_join(train_rf_stack) %>%
  left_join(train_lr_stack) %>%
  left_join(train_knn_stack) %>%
  select(-id)
```

```{r message=FALSE, warning=FALSE}
prediction_xgb_stack <- xgb_pred %>%
  select(id, xgb = .pred)

prediction_lgbm_stack <- lgbm_pred %>%
  select(id, lgbm = .pred)

prediction_rf_stack <- rf_pred %>%
  select(id, rf = .pred)

prediction_lr_stack <- lr_pred %>%
  select(id, lr = .pred)

prediction_knn_stack <- knn_pred %>%
  select(id, knn = .pred)

prediction_stack_data <- prediction_xgb_stack %>%
  left_join(prediction_lgbm_stack) %>%
  left_join(prediction_rf_stack) %>%
  left_join(prediction_lr_stack) %>%
  left_join(prediction_knn_stack) %>%
  select(-id)
```

### Meta Learner preparation

```{r message=FALSE, warning=FALSE}
# define a recipe for modeling
stack_model_recipe <- recipe(score ~ ., data = train_stack_data)

meta_lr_spec <- linear_reg(
  penalty = 0.009730234,
  mixture = 0.8191493
) %>%
  set_engine(engine = "glmnet") %>%
  set_mode("regression")

meta_lr_wf <- workflow() %>%
  add_recipe(stack_model_recipe) %>%
  add_model(meta_lr_spec) %>% 
  fit(train_stack_data)

```


## Predicting


```{r}
# predict with meta learner
pred_test <-  predict(meta_lr_wf, new_data = prediction_stack_data)
```


```{r eval=FALSE, include=FALSE}

sample_submission$score <- pred_test$.pred
write_csv(sample_submission, "submission.csv")
```



